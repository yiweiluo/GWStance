{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yiweiluo/scientific-debates\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "os.chdir('..')\n",
    "print(os.getcwd())\n",
    "from utils import get_fulltext,get_fname,fulltext_exists\n",
    "os.chdir('./data_processing/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>domain</th>\n",
       "      <th>stance</th>\n",
       "      <th>topic</th>\n",
       "      <th>is_AP</th>\n",
       "      <th>year</th>\n",
       "      <th>pretty_domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>www.buzzfeednews.com/article/tasneemnashrulla/...</td>\n",
       "      <td>\"eat the babies\" viral video at aoc town hall ...</td>\n",
       "      <td>2019-10-04 00:00:00</td>\n",
       "      <td>buzzfeed</td>\n",
       "      <td>pro</td>\n",
       "      <td>cc</td>\n",
       "      <td>None</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>www.buzzfeednews.com/article/passantino/extrem...</td>\n",
       "      <td>\"extremely likely\" global warming is man-made,...</td>\n",
       "      <td>2013-09-27 00:00:00</td>\n",
       "      <td>buzzfeed</td>\n",
       "      <td>pro</td>\n",
       "      <td>cc</td>\n",
       "      <td>None</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shotofprevention.com/2010/11/03/history-makes-...</td>\n",
       "      <td>\"history\" makes headlines with launch of new w...</td>\n",
       "      <td>2020-03-13 14:32:02</td>\n",
       "      <td>https://shotofprevention/</td>\n",
       "      <td>pro</td>\n",
       "      <td>vax</td>\n",
       "      <td>False</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>Shot of Prevention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>www.buzzfeednews.com/article/andrewkaczynski/i...</td>\n",
       "      <td>\"it's global warming, stupid\" - buzzfeed news</td>\n",
       "      <td>2012-11-01 00:00:00</td>\n",
       "      <td>buzzfeed</td>\n",
       "      <td>pro</td>\n",
       "      <td>cc</td>\n",
       "      <td>None</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>www.buzzfeednews.com/article/tasneemnashrulla/...</td>\n",
       "      <td>\"japan dropped an atomic bomb on america durin...</td>\n",
       "      <td>2014-02-24 00:00:00</td>\n",
       "      <td>buzzfeed</td>\n",
       "      <td>pro</td>\n",
       "      <td>cc</td>\n",
       "      <td>None</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>Buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>www.buzzfeednews.com/article/llevin/opinion-im...</td>\n",
       "      <td>\"look at my record, child\": joe biden showed m...</td>\n",
       "      <td>2019-10-31 00:00:00</td>\n",
       "      <td>buzzfeed</td>\n",
       "      <td>pro</td>\n",
       "      <td>cc</td>\n",
       "      <td>None</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>www.breitbart.com/politics/2019/06/14/orourke-...</td>\n",
       "      <td>\"president o'rourke will end oil and gas lease...</td>\n",
       "      <td>2019-06-14 00:00:00</td>\n",
       "      <td>breitbart</td>\n",
       "      <td>anti</td>\n",
       "      <td>cc</td>\n",
       "      <td>None</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>Breitbart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>www.buzzfeednews.com/article/andrewkaczynski/s...</td>\n",
       "      <td>smoking doesnt kill and other great old opeds ...</td>\n",
       "      <td>2015-03-31 00:00:00</td>\n",
       "      <td>buzzfeed</td>\n",
       "      <td>pro</td>\n",
       "      <td>cc</td>\n",
       "      <td>None</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Buzzfeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>www.foxnews.com/world/100-carbon-tax-by-2030-c...</td>\n",
       "      <td>$100 carbon tax by 2030 could save climate, sa...</td>\n",
       "      <td>2017-05-29 00:00:00</td>\n",
       "      <td>fox</td>\n",
       "      <td>anti</td>\n",
       "      <td>cc</td>\n",
       "      <td>None</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>Fox</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>childrenshealthdefense.org/news/4-billion-and-...</td>\n",
       "      <td>$4 billion and growing:  u.s. payouts for vacc...</td>\n",
       "      <td>2018-11-19 00:00:00</td>\n",
       "      <td>chd</td>\n",
       "      <td>anti</td>\n",
       "      <td>vax</td>\n",
       "      <td>None</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Children's Health Defense</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  \\\n",
       "0      www.buzzfeednews.com/article/tasneemnashrulla/...   \n",
       "1      www.buzzfeednews.com/article/passantino/extrem...   \n",
       "2      shotofprevention.com/2010/11/03/history-makes-...   \n",
       "3      www.buzzfeednews.com/article/andrewkaczynski/i...   \n",
       "4      www.buzzfeednews.com/article/tasneemnashrulla/...   \n",
       "5      www.buzzfeednews.com/article/llevin/opinion-im...   \n",
       "6      www.breitbart.com/politics/2019/06/14/orourke-...   \n",
       "7      www.buzzfeednews.com/article/andrewkaczynski/s...   \n",
       "8      www.foxnews.com/world/100-carbon-tax-by-2030-c...   \n",
       "21585  childrenshealthdefense.org/news/4-billion-and-...   \n",
       "\n",
       "                                                   title                 date  \\\n",
       "0      \"eat the babies\" viral video at aoc town hall ...  2019-10-04 00:00:00   \n",
       "1      \"extremely likely\" global warming is man-made,...  2013-09-27 00:00:00   \n",
       "2      \"history\" makes headlines with launch of new w...  2020-03-13 14:32:02   \n",
       "3          \"it's global warming, stupid\" - buzzfeed news  2012-11-01 00:00:00   \n",
       "4      \"japan dropped an atomic bomb on america durin...  2014-02-24 00:00:00   \n",
       "5      \"look at my record, child\": joe biden showed m...  2019-10-31 00:00:00   \n",
       "6      \"president o'rourke will end oil and gas lease...  2019-06-14 00:00:00   \n",
       "7      smoking doesnt kill and other great old opeds ...  2015-03-31 00:00:00   \n",
       "8      $100 carbon tax by 2030 could save climate, sa...  2017-05-29 00:00:00   \n",
       "21585  $4 billion and growing:  u.s. payouts for vacc...  2018-11-19 00:00:00   \n",
       "\n",
       "                          domain stance topic  is_AP    year  \\\n",
       "0                       buzzfeed    pro    cc   None  2019.0   \n",
       "1                       buzzfeed    pro    cc   None  2013.0   \n",
       "2      https://shotofprevention/    pro   vax  False  2020.0   \n",
       "3                       buzzfeed    pro    cc   None  2012.0   \n",
       "4                       buzzfeed    pro    cc   None  2014.0   \n",
       "5                       buzzfeed    pro    cc   None  2019.0   \n",
       "6                      breitbart   anti    cc   None  2019.0   \n",
       "7                       buzzfeed    pro    cc   None  2015.0   \n",
       "8                            fox   anti    cc   None  2017.0   \n",
       "21585                        chd   anti   vax   None  2018.0   \n",
       "\n",
       "                   pretty_domain  \n",
       "0                       Buzzfeed  \n",
       "1                       Buzzfeed  \n",
       "2             Shot of Prevention  \n",
       "3                       Buzzfeed  \n",
       "4                       Buzzfeed  \n",
       "5                       Buzzfeed  \n",
       "6                      Breitbart  \n",
       "7                       Buzzfeed  \n",
       "8                            Fox  \n",
       "21585  Children's Health Defense  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('../data_scraping/dedup_combined_df.pkl')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The president and the far right slammed Rep. Alexandria Ocasio-Cortez over the bizarre town hall speaker who suggested eating babies to solve the climate crisis. But the pro-Trump LaRouche PAC, which believes climate change is a hoax, said it planted the troll. President Donald Trump, his son Don Jr., and far-right conservatives attacked Democratic Rep. Alexandria Ocasio-Cortez after a speaker at her town hall meeting Thursday night went on a bizarre rant about how eating babies is the only solution to the climate crisis. But it turns out the stunt was staged by what is now a far-right pro-Trump conspiracy group that compares climate change activism to ‚Äúgenocide.‚Äù Trump called Ocasio-Cortez a ‚Äúwack job‚Äù on Twitter after his allies widely shared the video of the unidentified woman saying that Ocasio-Cortez‚Äôs Green New Deal was not enough to solve the climate crisis. AOC is a Wack Job! https://t.co/LU3hIeek0c  ‚ÄúWe‚Äôre not going to be here for much long because of the climate crisis,‚Äù the woman said, addressing Ocasio-Cortez. ‚ÄúI think your next campaign slogan has to be this: \\'We got to start eating babies. \\'‚Äù The woman then revealed her T-shirt, which read ‚ÄúSave the planet. Eat the children.‚Äù ‚ÄúEven if we were to bomb Russia, we still have too many people. Too much pollution,‚Äù the woman continued. ‚ÄúSo we have to get rid of the babies. That‚Äôs a big problem. Just stopping having babies is not enough. We need to eat the babies.‚Äù Ocasio-Cortez then attempted to pacify the woman and address the urgency of climate change while ignoring her ‚Äúeat the babies‚Äù comments. One of Ocasio-Cortez\\'s constituents loses her mind over climate change during AOC\\'s townhall, claims we only have a few months left: \"We got to start eating babies! We don\\'t have enough time! ... We have to get rid of the babies! ... We need to eat the babies!\" Trump supporters, including the president‚Äôs son, ridiculed the woman as a climate change activist. They were quick to characterize her as a typical Ocasio-Cortez supporter who, in their view, exemplified left-wing climate change hysteria. @AOC All of your supporters suffer from a mental condition. They also derided the Congress member for not attempting to stop the speaker‚Äôs rant and mischaracterized her apparent politeness as implicit support for the woman‚Äôs suggestion to eat babies. @AOC . @RepAOC, of course you‚Äôre referring to yourself, because you‚Äôre the one that has whipped up your supporters into thinking the world is coming to an end, so we have to start eating our babies now. Tucker Carlson blasts AOC over how she handled her supporter saying people must ‚Äústart eating babies!‚Äù: ‚ÄúIf someone said to you, ‚Äòwe need to eat the babies‚Äô wouldn‚Äôt your first response be, ‚Äòwhat? No! Of course not!‚Äô That‚Äôs the one thing [AOC] didn‚Äôt say ‚Ä¶ pretty revealing‚Äù  Ocasio-Cortez later tweeted that she was concerned the woman at the town hall was ‚Äúsuffering from a mental condition‚Äù and that she wanted to treat her with compassion. She also admonished right-wingers on Twitter for mocking the woman. After the video went viral, a group called LaRouche PAC ‚Äî which is affiliated with people with a long history of peddling unfounded conspiracy theories that has now turned pro-Trump ‚Äî took credit for planting the woman at the town hall meeting in an attempt to ‚Äútroll‚Äù Ocasio-Cortez and mock the climate change crisis. The fringe group, founded by conspiracy theorist Lyndon H. LaRouche Jr., who died earlier this year, believes climate change is a hoax and compares carbon dioxide reduction policies to ‚Äúgenocide.‚Äù ‚ÄúLaRouchePAC trolls AOC, AOC doesn\\'t rule out eating babies,‚Äù the group said on Twitter. LaRouchePAC trolls AOC, AOC doesn\\'t rule out eating babies. #EatTheBabies https://t.co/bD9ThZVcGO  Ocasio-Cortez later said in a tweet, ‚ÄúTurns out the woman yelling was a Trump supporter ü§∑üèΩ\\u200d‚ôÄÔ∏è.‚Äù The LaRouche PAC has been running an ‚ÄúEat the Children‚Äù campaign modeled on Jonathan Swift‚Äôs satirical 1729 essay A Modest Proposal, which attempted to highlight England‚Äôs exploitation of the impoverished Irish by suggesting that poor Irish families sell their children as food to wealthy landlords. In response to a tweet wondering if the town hall speaker was a plant, LaRouche PAC wrote, ‚ÄúIt was us,‚Äù adding, ‚ÄúMalthusianism isn‚Äôt new, Jonathan Swift knew that. Sometimes, only satire works.‚Äù Daniel Burke, who is affiliated with the LaRouche PAC, posted an ‚ÄúEat the babies‚Äù poster of Ocasio-Cortez on his Facebook account Thursday. Facebook / Daniel Burke  Burke had previously posted photos of the group‚Äôs ‚ÄúEat the Children‚Äù event in New York City on Tuesday, with signs that said ‚ÄúDefend Trump‚Äù and ‚ÄúThe New Green Deal Is Genocide For Africa.‚Äù Burke ‚Äî who says he is a LaRouche PAC‚Äìendorsed independent candidate for the US Senate ‚Äî bragged about Trump giving attention to its ‚ÄúEat the Children‚Äù stunt. ‚ÄúEarlier this week my friends and I went to NYU dressed up as Eat the Children!‚Äù Burke wrote on Facebook. ‚ÄúNow AOC gets a taste of Jonathan Swift, courtesy of LaRouche! Trump has retweeted! We\\'re over 2 Million views! Stop Green Genocide! It\\'s LUNACY!‚Äù Burke has previously claimed that the ‚ÄúObama cum ‚ÄòGreta Thunberg‚Äô phenomenon is an effort to create an international Green Nazi Youth‚Äù and that the outcome of a carbon dioxide reduction policy is ‚Äúnot less than genocide.‚Äù Matthew Sweet, a BBC Radio 3 presenter and author of Operation Chaos, which explores LaRouche‚Äôs conspiracy theories, first made the connection between the town hall troll and the group‚Äôs modus operandi. In a Twitter thread about the origins of the ‚Äúbizarre political cult‚Äù ‚Äî which believes the Queen controls the international drug trade ‚Äî Sweet explained that the LaRouche group has been pulling similar stunts for the past 50 years. Its targets have included Jane Fonda, Henry Kissinger, and former Democratic presidential candidate Michael Dukakis. ‚ÄúLaRouchies have been enemies of the green movement for decades,‚Äù Sweet wrote, adding that they carried signs like ‚ÄúFeed Jane Fonda to the whales.‚Äù Dukakis was forced to release his medical records during his 1988 presidential campaign after LaRouche‚Äôs followers spread unsubstantiated rumors about his mental health. ‚ÄúThe LaRouchies are now firm Trump supporters ‚Äî though Putin is their first love,‚Äù Sweet said. Sweet did not respond to a request for comment.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_fulltext(df.url.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents:\n",
    "* [Define functions](#Define-functions)\n",
    "* [Extract clausal complements with SpaCy](#Extract-embedded-clauses-with-SpaCy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a neural coref resolution step to the default SpaCy pipeline: https://github.com/huggingface/neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x133fa2f28>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# First way we can control a parameter\n",
    "#neuralcoref.add_to_pipe(nlp, greedyness=0.75)\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for the main ```get_quotes``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_good_verb_dep(dep):\n",
    "    return dep[:3] == 'aux' or dep[:3] == 'adv' or dep == 'det' or dep == 'rel' or dep == 'prep' or dep[-3:] == 'obj' or dep[-3:] == 'mod' or dep == 'prt' or (dep[-4:] == 'comp' and dep != 'ccomp')\n",
    "\n",
    "def is_good_subj_dep(dep):\n",
    "    return dep != 'ccomp'\n",
    "\n",
    "def is_ROOT(tok):\n",
    "    return tok.dep_ == 'ROOT' or tok.dep_[-2:] == 'cl' or tok.dep_ == 'ccomp' or \\\n",
    "            (tok.dep_ == 'conj' and tok.head.dep_ == 'ROOT')\n",
    "\n",
    "REL_PRONOUNS = set(['who', 'whom', 'whose', 'which', 'that'])\n",
    "def is_rel_pronoun(tok):\n",
    "    tok = tok.lower().strip()\n",
    "    return tok in REL_PRONOUNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_quotes(text):\n",
    "    # Do coref resolution\n",
    "    doc = nlp(text)\n",
    "    text = doc._.coref_resolved\n",
    "    \n",
    "    quote_objs = []\n",
    "    \n",
    "    for sent in sent_tokenize(text):#d.sents:\n",
    "        #print(sent)\n",
    "        sent = nlp(sent)\n",
    "        # Go through entire sentence, looking for verbs embedding a complement clause\n",
    "        VERBS = np.unique([token.head for token in sent if token.dep_ == 'ccomp'])\n",
    "        #print(VERBS)\n",
    "       \n",
    "        # Extract everything else for each VERB\n",
    "        for VERB in VERBS:\n",
    "            #print(\"\\nCcomp dependency found! For quoting verb '{}'\".format(VERB))\n",
    "            # Extract the rest of the quoting verb\n",
    "            verb_deps = [x for x in VERB.children if is_good_verb_dep(x.dep_)]\n",
    "            #print(\"\\tFound children:\",verb_deps)\n",
    "            for x in verb_deps:\n",
    "                new_children = [c for c in x.children if is_good_verb_dep(c.dep_)]\n",
    "                #print(\"\\tAdding children of {}:\".format(x.text),new_children)\n",
    "                verb_deps.extend(new_children)\n",
    "                #print(\"\\tUpdated verb deps:\",verb_deps)\n",
    "            \n",
    "            # If verb's head is not itself (i.e., verb is not the ROOT), \n",
    "            # recursively trace back to ROOT, then add all children of ROOT\n",
    "            ROOT = VERB\n",
    "            while not is_ROOT(ROOT):\n",
    "                ROOT = ROOT.head\n",
    "                #print('\\t\\tCurrent root:',ROOT)\n",
    "            if VERB is not ROOT:\n",
    "                verb_deps.append(ROOT) \n",
    "                \n",
    "            #print(\"\\tAdding children of ROOT...\")\n",
    "            root_deps = [x for x in ROOT.children if is_good_verb_dep(x.dep_)]\n",
    "            #print(\"\\tFound ROOT deps:\",root_deps)\n",
    "            for x in root_deps:\n",
    "                new_children = [c for c in x.children if is_good_verb_dep(c.dep_)]\n",
    "                #print(\"\\tAdding children of {}:\".format(x.text),new_children)\n",
    "                root_deps.extend(new_children)\n",
    "                #print(\"\\tUpdated ROOT deps:\",root_deps)\n",
    "\n",
    "            #print(\"\\tAdding ROOT deps to verb deps...\")\n",
    "            verb_deps.extend([x for x in root_deps if x != VERB and x not in verb_deps])\n",
    "            #print(\"\\tUpdated verb deps:\",verb_deps)\n",
    "            \n",
    "            NEG,IS_NEG,neg_children = None,None,None\n",
    "            SUBJECT,subj_children = None,None\n",
    "                \n",
    "            #print(\"\\nLooking for SUBJECT and NEGATION(s)...\")\n",
    "            for child in ROOT.children:\n",
    "                if child.dep_[:5] == 'nsubj' or child.dep_ == 'expl':\n",
    "                    SUBJECT = child\n",
    "                    #print(\"\\tFound SUBJECT:\",SUBJECT)\n",
    "                    if SUBJECT.head.dep_ == 'relcl' and is_rel_pronoun(SUBJECT.text): # we're dealing with the subject of a rel clause\n",
    "                        #print(\"\\tFound quote inside a relative clause. Finding antecedent subject...\")\n",
    "                        SUBJECT = SUBJECT.head.head\n",
    "                        #print(\"\\tTrue subject:\",SUBJECT)\n",
    "                    #print(\"Subject token '{}' is in a coref cluster:\".format(SUBJECT),SUBJECT._.in_coref)\n",
    "                \n",
    "                if child.dep_[:3] == 'neg':\n",
    "                    NEG = child\n",
    "                    verb_deps.append(NEG)\n",
    "                    neg_children = [c for c in NEG.children if c != VERB]\n",
    "                    #print(\"n\\tAdding new NEG children:\",neg_children)\n",
    "                    for x in neg_children:\n",
    "                        new_children = [c for c in x.children]\n",
    "                        #print(\"\\tNew NEG grandchildren:\",new_children)\n",
    "                        neg_children.extend(new_children)\n",
    "                        #print(\"\\tUpdated neg_children:\",neg_children)\n",
    "                    verb_deps.extend(neg_children)\n",
    "                    #print(\"\\tUpdated verb_deps:\",verb_deps)\n",
    "                    IS_NEG = VERB in NEG.head.children or VERB == NEG.head\n",
    "            \n",
    "            if SUBJECT is None and (ROOT.dep_ == 'acl' or ROOT.dep_ == 'advcl'):\n",
    "                main_verb = ROOT.head\n",
    "                #print([(c.text,c.dep_) for c in main_verb.children])\n",
    "                SUBJS = [c for c in main_verb.children if c.dep_[:5] == 'nsubj' or c.dep_ == 'expl']\n",
    "                SUBJECT = SUBJS[0] if len(SUBJS) > 0 else None\n",
    "           \n",
    "            # Get rest of subject tokens\n",
    "            if SUBJECT is not None:\n",
    "                #print(\"\\nFound SUBJECT:\",SUBJECT)\n",
    "                #print(\"\\tAdding children of SUBJECT...\")\n",
    "                subj_children = [c for c in SUBJECT.children if is_good_subj_dep(c.dep_)]\n",
    "                #print(\"\\tFound children:\",subj_children)\n",
    "                for x in subj_children:\n",
    "                    new_children = [c for c in x.children if is_good_subj_dep(c.dep_)]\n",
    "                    #print(\"\\tAdding children of child {}:\".format(x.text),new_children)\n",
    "                    subj_children.extend(new_children)\n",
    "                    #print(\"\\tUpdated subject children:\",subj_children)\n",
    "\n",
    "            sorted_verb_tokens = sorted([(c,c.i) for c in verb_deps+[VERB]],key=lambda x:x[1])\n",
    "            #print(\"\\n\\tSorted verb tokens:\",sorted_verb_tokens)\n",
    "            if SUBJECT is not None:\n",
    "                sorted_subj_tokens = sorted([(c,c.i) for c in subj_children+[SUBJECT]],key=lambda x:x[1])\n",
    "                #print(\"\\tSorted subject tokens:\",sorted_subj_tokens)\n",
    "            else:\n",
    "                sorted_subj_tokens = None\n",
    "            if NEG is not None:\n",
    "                sorted_neg_tokens = sorted([(c,c.i) for c in neg_children+[NEG]],key=lambda x:x[1])\n",
    "            else:\n",
    "                sorted_neg_tokens = None\n",
    "\n",
    "            #print(\"\\nFinding quote introduced by '{}'...\".format(VERB))\n",
    "            emb_main_verbs = [c for c in VERB.children if c.dep_ == 'ccomp']\n",
    "            #print(\"\\tMain verbs of embedded clause:\",emb_main_verbs)\n",
    "            #assert len(emb_main_verbs) <= 2\n",
    "            for emb_main_verb in emb_main_verbs:\n",
    "                #print(\"\\tMain *verb*:\",emb_main_verb)\n",
    "\n",
    "                # Recursively get all children of main verb of embedded clause \n",
    "                #print(\"\\nRecursively getting children of main verb of embedded clause...\\n\")\n",
    "                #print(\"*\"*50)\n",
    "                children_queue = [x for x in emb_main_verb.children]\n",
    "                #print(\"\\tAdding children of main verb:\",children_queue)\n",
    "                for x in children_queue:\n",
    "                    new_children = [c for c in x.children]\n",
    "                    #print(\"\\tAdding children of {}:\".format(x.text),new_children)\n",
    "                    children_queue.extend(new_children)\n",
    "                    #print(\"\\tNew children queue:\",children_queue)\n",
    "\n",
    "                # Sort children and matrix verb to be in correct order\n",
    "                #print(\"\\nSorting tokens in quote...\")\n",
    "                children_and_indices = [(c,c.i) for c in children_queue+[emb_main_verb]]\n",
    "                sorted_ = sorted(children_and_indices,key=lambda x:x[1])\n",
    "                quote_objs.append({'quote':[tup[0].text for tup in sorted_],\n",
    "                        'verb tokens':[tup[0].text for tup in sorted_verb_tokens],\n",
    "                        'main verb':VERB.text,\n",
    "                       'subject tokens':[tup[0].text for tup in sorted_subj_tokens] if sorted_subj_tokens is not None else None,\n",
    "                       'main subject':SUBJECT.text if SUBJECT is not None else None,\n",
    "                       'neg tokens':[tup[0].text for tup in sorted_neg_tokens] if sorted_neg_tokens is not None else None,\n",
    "                       'main neg':NEG.text if NEG is not None else None,\n",
    "                       'is neg':IS_NEG})\n",
    "    \n",
    "    return quote_objs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract clausal complements with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.651726722717285"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()-s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First 100 URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.buzzfeednews.com[SEP]article[SEP]tasneemnashrulla[SEP]eat-babies-aoc-town-hall-pro-trump-troll-larouche\tElapsed time in seconds: 0.05031239986419678\n"
     ]
    }
   ],
   "source": [
    "for url_ix in range(0,1):\n",
    "    start_time = time.time()\n",
    "    curr_url = df.url.values[url_ix]\n",
    "    quotes = get_quotes(get_fulltext(curr_url)[0])\n",
    "    fname = get_fname(curr_url)\n",
    "    with open('./extracted_quotes/{}.jsonlist'.format(fname),'w+') as f:\n",
    "        for res in quotes:\n",
    "            json.dump(res, f)\n",
    "            f.write('\\n')\n",
    "    print('{}\\tElapsed time in seconds:'.format(fname),(time.time()-start_time)/60.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_url_ix = 1\n",
    "curr_url = df.url.values[curr_url_ix]\n",
    "quotes = get_quotes(get_fulltext(curr_url)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.buzzfeednews.com[SEP]article[SEP]tasneemnashrulla[SEP]eat-babies-aoc-town-hall-pro-trump-troll-larouche\n"
     ]
    }
   ],
   "source": [
    "fname = get_fname(curr_url)\n",
    "print(fname)\n",
    "with open('./extracted_quotes/{}.jsonlist'.format(fname),'w+') as f:\n",
    "    for res in quotes:\n",
    "        json.dump(res, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "{'quote': ['climate', 'change', 'is', 'a', 'hoax'], 'verb tokens': ['believes'], 'main verb': 'believes', 'subject tokens': ['the', 'pro', '-', 'Trump', 'LaRouche', 'PAC', ',', 'which', 'believes', ','], 'main subject': 'PAC', 'neg tokens': None, 'main neg': None, 'is neg': None}\n"
     ]
    }
   ],
   "source": [
    "with open('./extracted_quotes/{}.jsonlist'.format(fname),'r') as f:\n",
    "    data = f.readlines()\n",
    "    print(len(data))\n",
    "    \n",
    "read_quotes = [json.loads(q.strip()) for q in data]\n",
    "print(read_quotes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quote': ['climate', 'change', 'is', 'a', 'hoax', 'planted', 'the', 'troll'],\n",
       " 'verb tokens': ['believes'],\n",
       " 'main verb': 'believes',\n",
       " 'subject tokens': ['the',\n",
       "  'pro',\n",
       "  '-',\n",
       "  'Trump',\n",
       "  'LaRouche',\n",
       "  'PAC',\n",
       "  ',',\n",
       "  'which',\n",
       "  'believes'],\n",
       " 'main subject': 'PAC',\n",
       " 'neg tokens': None,\n",
       " 'main neg': None,\n",
       " 'is neg': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_quotes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
