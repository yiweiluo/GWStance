{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertModel, BertSelfAttention\n",
    "import pytorch_pretrained_bert.modeling as modeling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    \"\"\"Implementation of the gelu activation function.\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "def identity(x):\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMultitask(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, cls_num_labels=2, tok_num_labels=2, tok2id=None):\n",
    "        super(BertForMultitask, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        self.cls_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.cls_classifier = nn.Linear(config.hidden_size, cls_num_labels)\n",
    "        \n",
    "        self.tok_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.tok_classifier = nn.Linear(config.hidden_size, tok_num_labels)\n",
    "        \n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, \n",
    "        labels=None, rel_ids=None, pos_ids=None, categories=None, pre_len=None):\n",
    "        global ARGS\n",
    "        sequence_output, pooled_output = self.bert(\n",
    "            input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "\n",
    "        cls_logits = self.cls_classifier(pooled_output)\n",
    "        cls_logits = self.cls_dropout(cls_logits)\n",
    "\n",
    "        # NOTE -- dropout is after proj, which is non-standard\n",
    "        tok_logits = self.tok_classifier(sequence_output)\n",
    "        tok_logits = self.tok_dropout(tok_logits)\n",
    "\n",
    "        return cls_logits, tok_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class ConcatCombine(nn.Module):\n",
    "    def __init__(self, hidden_size, feature_size, out_size, layers,\n",
    "            dropout_prob, small=False, pre_enrich=False, activation=False,\n",
    "            include_categories=False, category_emb=False,\n",
    "            add_category_emb=False):\n",
    "        super(ConcatCombine, self).__init__()\n",
    "\n",
    "        self.include_categories = include_categories\n",
    "        self.add_category_emb = add_category_emb\n",
    "        if include_categories:\n",
    "            if category_emb and not add_category_emb:\n",
    "                feature_size *= 2\n",
    "            elif not category_emb:\n",
    "                feature_size += 43\n",
    "\n",
    "        if layers == 1:\n",
    "            self.out = nn.Sequential(\n",
    "                nn.Linear(hidden_size + feature_size, out_size),\n",
    "                nn.Dropout(dropout_prob))\n",
    "        elif layers == 2:\n",
    "            waist_size = min(hidden_size, feature_size) if small else max(hidden_size, feature_size)\n",
    "            if activation:\n",
    "                self.out = nn.Sequential(\n",
    "                    nn.Linear(hidden_size + feature_size, waist_size),\n",
    "                    nn.Dropout(dropout_prob),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(waist_size, out_size),\n",
    "                    nn.Dropout(dropout_prob))\n",
    "            else:\n",
    "                self.out = nn.Sequential(\n",
    "                    nn.Linear(hidden_size + feature_size, waist_size),\n",
    "                    nn.Dropout(dropout_prob),\n",
    "                    nn.Linear(waist_size, out_size),\n",
    "                    nn.Dropout(dropout_prob))\n",
    "        if pre_enrich:\n",
    "            if activation:\n",
    "                self.enricher = nn.Sequential(\n",
    "                    nn.Linear(feature_size, feature_size),\n",
    "                    nn.ReLU())\n",
    "            else:\n",
    "                self.enricher = nn.Linear(feature_size, feature_size)\n",
    "        else:\n",
    "            self.enricher = None\n",
    "        # manually set cuda because module doesn't see these combiners for bottom \n",
    "        if CUDA:\n",
    "            self.out = self.out.cuda()\n",
    "            if self.enricher: \n",
    "                self.enricher = self.enricher.cuda()\n",
    "                \n",
    "    def forward(self, hidden, features, categories=None):\n",
    "        if self.include_categories:\n",
    "            categories = categories.unsqueeze(1)\n",
    "            categories = categories.repeat(1, features.shape[1], 1)\n",
    "            if self.add_category_emb:\n",
    "                features = features + categories\n",
    "            else:\n",
    "                features = torch.cat((features, categories), -1)\n",
    "\n",
    "        if self.enricher is not None:\n",
    "            features = self.enricher(features)\n",
    "\n",
    "        return self.out(torch.cat((hidden, features), dim=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AddCombine(nn.Module):\n",
    "    def __init__(self, hidden_dim, feat_dim, layers, dropout_prob, small=False,\n",
    "            out_dim=-1, pre_enrich=False, include_categories=False,\n",
    "            category_emb=False, add_category_emb=False):\n",
    "        super(AddCombine, self).__init__()\n",
    "\n",
    "        self.include_categories = include_categories\n",
    "        if include_categories:\n",
    "            feat_dim += 43\n",
    "\n",
    "        if layers == 1:\n",
    "            self.expand = nn.Sequential(\n",
    "                nn.Linear(feat_dim, hidden_dim),\n",
    "                nn.Dropout(dropout_prob))\n",
    "        else:\n",
    "            waist_size = min(feat_dim, hidden_dim) if small else max(feat_dim, hidden_dim)\n",
    "            self.expand = nn.Sequential(\n",
    "                nn.Linear(feat_dim, waist_size),\n",
    "                nn.Dropout(dropout_prob),\n",
    "                nn.Linear(waist_size, hidden_dim),\n",
    "                nn.Dropout(dropout_prob))\n",
    "        \n",
    "        if out_dim > 0:\n",
    "            self.out = nn.Linear(hidden_dim, out_dim)\n",
    "        else:\n",
    "            self.out = None\n",
    "\n",
    "        if pre_enrich:\n",
    "            self.enricher = nn.Linear(feature_size, feature_size)        \n",
    "        else:\n",
    "            self.enricher = None\n",
    "\n",
    "        # manually set cuda because module doesn't see these combiners for bottom         \n",
    "        if CUDA:\n",
    "            self.expand = self.expand.cuda()\n",
    "            if out_dim > 0:\n",
    "                self.out = self.out.cuda()\n",
    "            if self.enricher is not None:\n",
    "                self.enricher = self.enricher.cuda()\n",
    "\n",
    "    def forward(self, hidden, feat, categories=None):\n",
    "        if self.include_categories:\n",
    "            categories = categories.unsqueeze(1)\n",
    "            categories = categories.repeat(1, features.shape[1], 1)\n",
    "            if self.add_category_emb:\n",
    "                features = features + categories\n",
    "            else:\n",
    "                features = torch.cat((features, categories), -1)\n",
    "\n",
    "        if self.enricher is not None:\n",
    "            feat = self.enricher(feat)\n",
    "    \n",
    "        combined = self.expand(feat) + hidden\n",
    "    \n",
    "        if self.out is not None:\n",
    "            return self.out(combined)\n",
    "\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class BertForMultitaskWithFeaturesOnTop(BertPreTrainedModel):\n",
    "    \"\"\" stick the features on top of the model \"\"\"\n",
    "    def __init__(self, config, cls_num_labels=2, tok_num_labels=2, tok2id=None):\n",
    "        super(BertForMultitaskWithFeaturesOnTop, self).__init__(config)\n",
    "        global ARGS\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        self.featurizer = features.Featurizer(\n",
    "            tok2id, lexicon_feature_bits=ARGS.lexicon_feature_bits) \n",
    "        # TODO -- don't hardcode this...\n",
    "        nfeats = 90 if ARGS.lexicon_feature_bits == 1 else 118\n",
    "\n",
    "        if ARGS.extra_features_method == 'concat':\n",
    "            self.tok_classifier = ConcatCombine(\n",
    "                config.hidden_size, nfeats, tok_num_labels, \n",
    "                ARGS.combiner_layers, config.hidden_dropout_prob,\n",
    "                ARGS.small_waist, pre_enrich=ARGS.pre_enrich,\n",
    "                activation=ARGS.activation_hidden,\n",
    "                include_categories=ARGS.concat_categories,\n",
    "                category_emb=ARGS.category_emb,\n",
    "                add_category_emb=ARGS.add_category_emb)\n",
    "        else:\n",
    "            self.tok_classifier = AddCombine(\n",
    "                config.hidden_size, nfeats, ARGS.combiner_layers,\n",
    "                config.hidden_dropout_prob, ARGS.small_waist,\n",
    "                out_dim=tok_num_labels, pre_enrich=ARGS.pre_enrich,\n",
    "                include_categories=ARGS.concat_categories,\n",
    "                category_emb=ARGS.category_emb,\n",
    "                add_category_emb=ARGS.add_category_emb)\n",
    "\n",
    "        self.cls_dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.cls_classifier = nn.Linear(config.hidden_size, cls_num_labels)\n",
    "\n",
    "        self.category_emb = ARGS.category_emb\n",
    "        if ARGS.category_emb:\n",
    "            self.category_embeddings = nn.Embedding(43, nfeats)\n",
    "\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, \n",
    "        labels=None, rel_ids=None, pos_ids=None, categories=None, pre_len=None):\n",
    "        global ARGS\n",
    "        global CUDA\n",
    "\n",
    "        features = self.featurizer.featurize_batch(\n",
    "            input_ids.detach().cpu().numpy(), \n",
    "            rel_ids.detach().cpu().numpy(), \n",
    "            pos_ids.detach().cpu().numpy(), \n",
    "            padded_len=input_ids.shape[1])\n",
    "        features = torch.tensor(features, dtype=torch.float)\n",
    "        if CUDA:\n",
    "            features = features.cuda()\n",
    "\n",
    "        sequence_output, pooled_output = self.bert(\n",
    "            input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "\n",
    "        pooled_output = self.cls_dropout(pooled_output)\n",
    "        cls_logits = self.cls_classifier(pooled_output)\n",
    "\n",
    "        if ARGS.category_emb:\n",
    "            categories = self.category_embeddings(\n",
    "                categories.max(-1)[1].type(\n",
    "                    'torch.cuda.LongTensor' if CUDA else 'torch.LongTensor'))\n",
    "\n",
    "        tok_logits = self.tok_classifier(sequence_output, features, categories)\n",
    "\n",
    "        return cls_logits, tok_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer and fine-tuned LM (my-cc, uncased) + trained-vanilla\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../BERT/trained_models/vanilla/uncased_LM/uncased\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODELS_DIR = '../BERT/trained_models'\n",
    "DATA_NAME = 'vanilla'\n",
    "BASE_MOD = 'uncased_LM'\n",
    "CASING = 'uncased'\n",
    "model_path = os.path.join(PRETRAINED_MODELS_DIR,DATA_NAME,BASE_MOD,\n",
    "                         CASING)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data_creation/scripts/save/vanilla'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dev_set = pd.read_csv(os.path.join(DATA_DIR,'dev.tsv'),\n",
    "                          sep='\\t',header=None)\n",
    "eval_dev_set.columns = ['text','label','outlet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>outlet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The fall-off in ice volume is so fast it is go...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>More warming leads to more fires, which releas...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just to drive home to people what that means, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gradual loss of these largely pristine, intact...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The worst-case scenario would be the creation ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>The theory that carbon dioxide is a pollutant ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Global temperatures today are the same as they...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Carbon dioxide emissions provide the environme...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Young people like Thunberg are simply being us...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Climate alarmists necessarily demonize America...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label  outlet\n",
       "0    The fall-off in ice volume is so fast it is go...      0       1\n",
       "1    More warming leads to more fires, which releas...      0       0\n",
       "2    Just to drive home to people what that means, ...      0       1\n",
       "3    Gradual loss of these largely pristine, intact...      0       1\n",
       "4    The worst-case scenario would be the creation ...      0       1\n",
       "..                                                 ...    ...     ...\n",
       "135  The theory that carbon dioxide is a pollutant ...      2       0\n",
       "136  Global temperatures today are the same as they...      2       0\n",
       "137  Carbon dioxide emissions provide the environme...      2       0\n",
       "138  Young people like Thunberg are simply being us...      2       0\n",
       "139  Climate alarmists necessarily demonize America...      2       0\n",
       "\n",
       "[140 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dev_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive_file: ../BERT/LM_finetuned/uncased_LM_cc_output/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(model_path, num_labels=3)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,\n",
    "                                                          config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (os.path.join(model_path,'vocab.txt'),'r') as f:\n",
    "    vocab = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [l.strip() for l in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = eval_dev_set.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fall-off in ice volume is so fast it is going to bring us to zero very quickly.\n",
      "More warming leads to more fires, which release more carbon, which causes more warming, and so on.\n",
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for dev_ix in range(0,2):\n",
    "    sent = to_predict[dev_ix]\n",
    "    print(sent)\n",
    "    label = eval_dev_set.label.values[dev_ix]\n",
    "    encoded_sent = tokenizer.encode(sent,add_special_tokens=True)\n",
    "    out['input_ids'].append(encoded_sent)\n",
    "    out['sentences'].append(sent)\n",
    "    out['labels'].append(label)\n",
    "    \n",
    "out['input_ids'] = pad_sequences(\n",
    "        out['input_ids'], \n",
    "        maxlen=128, \n",
    "        dtype=\"long\", \n",
    "        value=0, \n",
    "        truncating=\"post\", \n",
    "        padding=\"post\")\n",
    "\n",
    "# get attn masks\n",
    "for sent in out['input_ids']:\n",
    "    tok_type_ids = [0 for tok_id in sent]\n",
    "    mask = [int(tok_id > 0) for tok_id in sent]\n",
    "    out['attention_mask'].append(mask)\n",
    "    out['token_type_ids'].append(tok_type_ids)\n",
    "print(len(out['labels']))\n",
    "print(sum(out['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "objs = [{'input_ids':torch.LongTensor([list(out['input_ids'][ix])]),\n",
    "        'token_type_ids':torch.LongTensor([out['token_type_ids'][ix]]),\n",
    "        'attention_mask':torch.LongTensor([out['attention_mask'][ix]])} \n",
    "        for ix in range(len(out['input_ids']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed processed 'out' to model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modeled_logits = [model(**objs[ix])[0] for ix in range(len(objs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeled_results = [torch.softmax(x, dim=1).tolist()[0] \n",
    "                  for x in modeled_logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = [get_pred_label(x) for x in modeled_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "import math\n",
    "\n",
    "CUDA = (torch.cuda.device_count() > 0)\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# # Required parameters\n",
    "# parser.add_argument(\n",
    "#     \"--max_seq_len\",\n",
    "#     default=512,\n",
    "#     type=int,\n",
    "#     help=\"max seq len\"\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--context_size\",\n",
    "#     default=0,\n",
    "#     type=int,\n",
    "#     help=\"num messages to include in context\"\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--working_dir\",\n",
    "#     default='working_dir',\n",
    "#     type=str,\n",
    "#     help=\"num messages to include in context\"\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--epochs\",\n",
    "#     default=70,\n",
    "#     type=int,\n",
    "#     help=\"fine tuning epochs\"\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--batch_size\",\n",
    "#     default=10,\n",
    "#     type=int,\n",
    "#     help=\"fine tuning epochs\"\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--learning_rate\",\n",
    "#     default=2e-5,\n",
    "#     type=int,\n",
    "#     help=\"fine tuning epochs\"\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--seed\",\n",
    "#     default=420,\n",
    "#     type=int,\n",
    "#     help=\"fine tuning epochs\"\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--length_discard\",\n",
    "#     action='store_true',\n",
    "#     help=\"discard examples that are too long\"\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--include_metadata\",\n",
    "#     action='store_true',\n",
    "#     help=\"discard examples that are too long\"\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"--downsample\",\n",
    "#     default=0.2,\n",
    "#     type=float,\n",
    "#     help=\"p = prop examples to throw out\"\n",
    "# )\n",
    "# ARGS = parser.parse_args()\n",
    "\n",
    "\n",
    "# random.seed(ARGS.seed)\n",
    "# np.random.seed(ARGS.seed)\n",
    "# torch.manual_seed(ARGS.seed)\n",
    "# torch.cuda.manual_seed_all(ARGS.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['for','against','neutral']\n",
    "NUM_LABELS = 3\n",
    "\n",
    "def get_pred_label(res_,to_str=False):\n",
    "    if to_str:\n",
    "        return CLASSES[res_.index(max(res_))]\n",
    "    else:\n",
    "        return res_.index(max(res_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_creation/scripts/save/mturk_windowed_1_downsampled\n",
      "../BERT/LM_finetuned/uncased_LM_cc_output\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODELS_DIR = '../BERT/trained_models'\n",
    "\n",
    "DATA_NAME = 'mturk_windowed_1_downsampled'\n",
    "BASE_MOD = 'uncased_LM'\n",
    "CASING = 'uncased'\n",
    "DATA_DIR = os.path.join('../data_creation/scripts/save',DATA_NAME)\n",
    "print(DATA_DIR)\n",
    "\n",
    "#model_path = os.path.join(PRETRAINED_MODELS_DIR,DATA_NAME,BASE_MOD,\n",
    "#                         CASING)\n",
    "model_path = '../BERT/LM_finetuned/uncased_LM_cc_output'\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_out_data(dat_path,max_seq_length=500):\n",
    "    #eval_set = 'train' # can also be 'test'\n",
    "    data = pd.read_csv(dat_path,\n",
    "                              sep='\\t',header=None)\n",
    "    data.columns = ['text','label']#,'outlet']\n",
    "    \n",
    "    out = defaultdict(list)\n",
    "    \n",
    "    print('Number of examples to predict:',len(data))\n",
    "    to_predict = data.text.values\n",
    "    true = data.label.values\n",
    "    \n",
    "    for dat_ix in range(len(data)):\n",
    "        sent = to_predict[dat_ix]\n",
    "        #print(sent)\n",
    "        label = true[dat_ix]\n",
    "        encoded_sent = tokenizer.encode(sent,add_special_tokens=True)\n",
    "        out['input_ids'].append(encoded_sent)\n",
    "        out['sentences'].append(sent)\n",
    "        out['label'].append(label)\n",
    "\n",
    "    out['input_ids'] = pad_sequences(\n",
    "            out['input_ids'], \n",
    "            maxlen=max_seq_length, \n",
    "            dtype=\"long\", \n",
    "            value=0, \n",
    "            truncating=\"post\", \n",
    "            padding=\"post\")\n",
    "\n",
    "\n",
    "    print('Adding attention masks...')\n",
    "    # get attn masks\n",
    "    for sent in out['input_ids']:\n",
    "        tok_type_ids = [0 for tok_id in sent]\n",
    "        mask = [int(tok_id > 0) for tok_id in sent]\n",
    "        out['attention_mask'].append(mask)\n",
    "        out['token_type_ids'].append(tok_type_ids)\n",
    "    #print(len(out['labels']))\n",
    "    #print(sum(out['labels']))\n",
    "    \n",
    "    print('Preparing input examples for prediction...')\n",
    "    #out['input_ids'] = [torch.LongTensor(x) for x in out['input_ids']]\n",
    "#     objs = [{'input_ids':torch.LongTensor([list(out['input_ids'][ix])]),\n",
    "#         'token_type_ids':torch.LongTensor([out['token_type_ids'][ix]]),\n",
    "#         'attention_mask':torch.LongTensor([out['attention_mask'][ix]]),\n",
    "#             'label':torch.LongTensor([out['label'][ix]])} \n",
    "#         for ix in range(len(out['input_ids']))]\n",
    "\n",
    "#     objs = [{'input_ids':[list(out['input_ids'][ix])],\n",
    "#         'token_type_ids':[out['token_type_ids'][ix]],\n",
    "#         'attention_mask':[out['attention_mask'][ix]],\n",
    "#             'label':[out['label'][ix]]} \n",
    "#         for ix in range(len(out['input_ids']))]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = '.'\n",
    "writer = SummaryWriter(WORKING_DIR + '/events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive_file: ../BERT/LM_finetuned/uncased_LM_cc_output/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "config = BertConfig.from_pretrained(model_path, num_labels=NUM_LABELS)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,\n",
    "                                                          config=config)\n",
    "with open(os.path.join(model_path,'vocab.txt'),'r') as f:\n",
    "    vocab = f.readlines()\n",
    "vocab = [l.strip() for l in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples to predict: 823\n",
      "Adding attention masks...\n",
      "Preparing input examples for prediction...\n"
     ]
    }
   ],
   "source": [
    "# if os.path.exists(WORKING_DIR + \"/data.cache.pkl\"):\n",
    "#     data = pickle.load(open(WORKING_DIR + \"/data.cache.pkl\", 'rb'))\n",
    "# else:\n",
    "data = get_out_data(os.path.join(DATA_DIR, 'train.tsv'))\n",
    "pickle.dump(data, open(WORKING_DIR + \"/data.cache.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_inputs, test_inputs, train_labels, test_labels, train_masks, test_masks, = train_test_split(\n",
    "    data['input_ids'], data['label'], data['attention_mask'],\n",
    "    random_state=SEED, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_dataloader(*args, sampler='random'):\n",
    "    #print(args[:2])\n",
    "    data = (torch.tensor(x) for x in args)\n",
    "    #print(data[0])\n",
    "    data = TensorDataset(*data)\n",
    "\n",
    "    #sampler = RandomSampler(data) if sampler == 'random' else SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, batch_size=1)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = build_dataloader(\n",
    "    train_inputs[:100], train_labels[:100], train_masks[:100])\n",
    "test_dataloader = build_dataloader(\n",
    "    test_inputs, test_labels, test_masks,\n",
    "    sampler='order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE=2e-5\n",
    "optimizer = AdamW(model.parameters(), lr=LEARN_RATE, eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS=1\n",
    "total_steps = len(train_dataloader) * NUM_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.60\n",
      "  Training epcoh took: 0:05:23\n",
      "\n",
      "Running Validation...\n",
      "  Loss: 0.60\n",
      "  Accuracy: 0.42\n",
      "  F1: 0.26\n",
      "  Validation took: 0:05:23\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for epoch_i in range(0, NUM_EPOCHS):\n",
    "    \n",
    "#     # ========================================\n",
    "#     #               Training\n",
    "#     # ========================================\n",
    "#     print(\"\")\n",
    "#     print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, NUM_EPOCHS))\n",
    "#     print('Training...')\n",
    "\n",
    "#     losses = []\n",
    "#     t0 = time.time()\n",
    "#     model.train()\n",
    "#     for step, batch in enumerate(train_dataloader):\n",
    "#         #print(step,batch)\n",
    "\n",
    "#         if step % 40 == 0 and not step == 0:\n",
    "#             elapsed = format_time(time.time() - t0)\n",
    "#             print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}. Loss: {:.2f}'.format(\n",
    "#                 step, len(train_dataloader), elapsed, float(np.mean(losses))))\n",
    "\n",
    "#         if CUDA:\n",
    "#             batch = (x.cuda() for x in batch)            \n",
    "#         input_ids, labels, masks = batch\n",
    "#         model.zero_grad()        \n",
    "\n",
    "#         outputs = model(\n",
    "#             input_ids,\n",
    "#             attention_mask=masks, \n",
    "#             labels=labels)\n",
    "        \n",
    "#         print(len(outputs))\n",
    "        \n",
    "#         #loss, _, _ = outputs\n",
    "#         loss, _ = outputs\n",
    "#         losses.append(loss.item())\n",
    "\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "\n",
    "#     avg_loss = np.mean(losses)\n",
    "#     writer.add_scalar('train/loss', np.mean(avg_loss), epoch_i)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "#     t0 = time.time()\n",
    "#     model.eval()\n",
    "#     losses = []\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "#     log = open(WORKING_DIR + '/epoch%d.log' % epoch_i, 'w')\n",
    "#     for step, batch in enumerate(test_dataloader):\n",
    "\n",
    "#         if CUDA:\n",
    "#             batch = (x.cuda() for x in batch)            \n",
    "#         input_ids, labels, masks = batch\n",
    "\n",
    "#         with torch.no_grad():        \n",
    "#             outputs = model(\n",
    "#                 input_ids,\n",
    "#                 attention_mask=masks, \n",
    "#                 labels=labels)\n",
    "#         #loss, logits, attns = outputs\n",
    "#         loss, logits = outputs\n",
    "\n",
    "#         losses.append(loss.item())\n",
    "\n",
    "#         labels = labels.cpu().numpy()\n",
    "#         input_ids = input_ids.cpu().numpy()\n",
    "#         preds = scipy.special.softmax(logits.cpu().numpy(), axis=1)\n",
    "#         input_toks = [\n",
    "#             tokenizer.convert_ids_to_tokens(s) for s in input_ids\n",
    "#         ]\n",
    "\n",
    "#         for seq, label, pred in zip(input_toks, labels, preds):\n",
    "#             sep_char = '+' if np.argmax(pred) == label else '-'\n",
    "#             log.write(sep_char * 40 + '\\n')\n",
    "#             log.write(' '.join(seq) + '\\n')\n",
    "#             log.write('label: ' + str(label) + '\\n')\n",
    "#             log.write('pred: ' + str(np.argmax(pred)) + '\\n')\n",
    "#             log.write('dist: ' + str(pred) + '\\n')\n",
    "#             log.write('\\n\\n')\n",
    "\n",
    "#             all_preds += [pred]\n",
    "#             all_labels += [label]\n",
    "#     log.close()\n",
    "#     all_preds = np.array(all_preds)\n",
    "#     all_labels = np.array(all_labels)\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    f1 = sklearn.metrics.f1_score(all_labels, np.argmax(all_preds, axis=1),average='macro')\n",
    "    acc = sklearn.metrics.accuracy_score(all_labels, np.argmax(all_preds, axis=1))\n",
    "    #auc = sklearn.metrics.roc_auc_score(all_labels, all_preds[:, 1])\n",
    "\n",
    "    writer.add_scalar('eval/acc', acc, epoch_i)\n",
    "    #writer.add_scalar('eval/auc', auc, epoch_i)\n",
    "    writer.add_scalar('eval/f1', f1, epoch_i)\n",
    "    writer.add_scalar('eval/loss', f1, epoch_i)\n",
    "\n",
    "    print(\"  Loss: {0:.2f}\".format(avg_loss))\n",
    "    print(\"  Accuracy: {0:.2f}\".format(acc))\n",
    "    print(\"  F1: {0:.2f}\".format(f1))\n",
    "    #print(\"  AUC: {0:.2f}\".format(auc))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
